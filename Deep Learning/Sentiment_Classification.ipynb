{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spela_Zavodnik_Assignment 2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5RudM9wBzd9",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 2\n",
        "\n",
        "(Part of this exercise is based on https://blog.conceptnet.io/posts/2017/how-to-make-a-racist-ai-without-really-trying/)\n",
        "\n",
        "## Sentiment Classification\n",
        "\n",
        "This assignment approaches the sentiment classification problem by using deep learning techniques and also exposes some ethical problem related to these methods.\n",
        "\n",
        "This is what we’re going to do:\n",
        "\n",
        "+ Acquire some pre-computed word embeddings to represent the meanings of words\n",
        "+ Acquire training and test data, with gold-standard examples of positive and negative words\n",
        "+ Train a simple classifier to recognize other positive and negative words based on their word embeddings\n",
        "+ Compute sentiment scores for sentences of text using this classifier\n",
        "+ Analyze the results to look for unwanted bias. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsS5s_tEBzd_",
        "colab_type": "text"
      },
      "source": [
        "## Word embeddings\n",
        "\n",
        "There are several datasets of pre-trained English word embeddings such as `word2vec`, pretrained on Google News data, and `GloVe`, pretrained on the Common Crawl of web pages. We will use `GloVe`.\n",
        "\n",
        "GloVe comes in three sizes: 6B, 42B, and 840B. The 42B version is pretty good and is also neatly trimmed to a vocabulary of 1 million words. We will just use the 42B version.\n",
        "\n",
        "> **GloVe.42B** data: 42B tokens, 1.9M vocab, uncased, 300d vectors, 1.75 GB download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXRJque9Pd80",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "a6bb1b97-4fc4-40c0-dbda-1523e02651cd"
      },
      "source": [
        "!pip install numpy==1.16.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy==1.16.2 in /usr/local/lib/python3.6/dist-packages (1.16.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-29T09:08:49.211876Z",
          "start_time": "2019-03-29T09:01:34.100348Z"
        },
        "id": "cEqMSox2BzeA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "c141b243-68a0-4d32-f7ab-87e7c76f6c5d"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.42B.300d.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-30 10:14:03--  http://nlp.stanford.edu/data/glove.42B.300d.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.42B.300d.zip [following]\n",
            "--2019-04-30 10:14:03--  https://nlp.stanford.edu/data/glove.42B.300d.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1877800501 (1.7G) [application/zip]\n",
            "Saving to: ‘glove.42B.300d.zip’\n",
            "\n",
            "glove.42B.300d.zip  100%[===================>]   1.75G  12.2MB/s    in 2m 50s  \n",
            "\n",
            "2019-04-30 10:16:53 (10.5 MB/s) - ‘glove.42B.300d.zip’ saved [1877800501/1877800501]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-29T09:10:02.327170Z",
          "start_time": "2019-03-29T09:09:24.646719Z"
        },
        "id": "aLvMgbhYBzeE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "c9aa57c4-4f82-4fb9-875f-aa4c7c7b3ed4"
      },
      "source": [
        "!unzip glove.42B.300d.zip\n",
        "!rm glove.42B.300d.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.42B.300d.zip\n",
            "replace glove.42B.300d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: glove.42B.300d.txt      y\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-29T15:04:47.628009Z",
          "start_time": "2019-03-29T15:01:58.108263Z"
        },
        "id": "G_FFD2bbBzeH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "9b01277c-feb4-4f89-dd97-f6f2b63a5476"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "def load_embeddings(filename):\n",
        "    labels = []\n",
        "    rows = []\n",
        "    with open(filename, encoding='utf-8') as infile:\n",
        "        for i, line in tqdm(enumerate(infile)):\n",
        "            items = line.rstrip().split(' ')\n",
        "            if len(items) == 2:\n",
        "                # This is a header row giving the shape of the matrix\n",
        "                continue\n",
        "            labels.append(items[0])\n",
        "            values = np.array([float(x) for x in items[1:]], 'f')\n",
        "            rows.append(values)\n",
        "    \n",
        "    arr = np.vstack(rows)\n",
        "    return pd.DataFrame(arr, index=labels, dtype='f')\n",
        "\n",
        "embeddings = load_embeddings('glove.42B.300d.txt')\n",
        "embeddings.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1917494it [03:44, 8546.21it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1917494, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fR6RR0h9BzeJ",
        "colab_type": "text"
      },
      "source": [
        "## Positive and Negative Words\n",
        "\n",
        "We need some input about which words are positive and which words are negative. There are many sentiment lexicons you could use, but we’re going to go with a very straightforward lexicon from https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html  \n",
        "\n",
        "There is a copy of these files in the GitHub repository of the course."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOVI_gAAE9OB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "c972d84c-673d-44e3-9f52-e4dda3a5a0fe"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/DataScienceUB/DeepLearningMaster2019/master/data/positive-words.txt\n",
        "!wget https://raw.githubusercontent.com/DataScienceUB/DeepLearningMaster2019/master/data/negative-words.txt\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-30 10:23:51--  https://raw.githubusercontent.com/DataScienceUB/DeepLearningMaster2019/master/data/positive-words.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22408 (22K) [text/plain]\n",
            "Saving to: ‘positive-words.txt.1’\n",
            "\n",
            "\rpositive-words.txt.   0%[                    ]       0  --.-KB/s               \rpositive-words.txt. 100%[===================>]  21.88K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2019-04-30 10:23:51 (2.61 MB/s) - ‘positive-words.txt.1’ saved [22408/22408]\n",
            "\n",
            "--2019-04-30 10:23:53--  https://raw.githubusercontent.com/DataScienceUB/DeepLearningMaster2019/master/data/negative-words.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 50857 (50K) [text/plain]\n",
            "Saving to: ‘negative-words.txt.1’\n",
            "\n",
            "negative-words.txt. 100%[===================>]  49.67K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2019-04-30 10:23:53 (2.00 MB/s) - ‘negative-words.txt.1’ saved [50857/50857]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-29T15:04:56.665741Z",
          "start_time": "2019-03-29T15:04:56.619197Z"
        },
        "id": "u6kfpHJaBzeJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_lexicon(filename):\n",
        "    lexicon = []\n",
        "    with open(filename, encoding='latin-1') as infile:\n",
        "        for line in infile:\n",
        "            line = line.rstrip()\n",
        "            if line and not line.startswith(';'):\n",
        "                lexicon.append(line)\n",
        "    return lexicon\n",
        "\n",
        "pos_words = load_lexicon('positive-words.txt')\n",
        "neg_words = load_lexicon('negative-words.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud0fhRxyBzeM",
        "colab_type": "text"
      },
      "source": [
        "Some of these words are not in the GloVe vocabulary. Those words end up with rows full of NaN to indicate their missing embeddings, so we will use Pandas to clean the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-29T15:04:59.940580Z",
          "start_time": "2019-03-29T15:04:58.168200Z"
        },
        "id": "mW5w55RiBzeM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "bd6f851e-3996-495d-f53b-8044853de091"
      },
      "source": [
        "pos_vectors = embeddings.loc[pos_words].dropna()\n",
        "neg_vectors = embeddings.loc[neg_words].dropna()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: \n",
            "Passing list-likes to .loc or [] with any missing label will raise\n",
            "KeyError in the future, you can use .reindex() as an alternative.\n",
            "\n",
            "See the documentation here:\n",
            "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: FutureWarning: \n",
            "Passing list-likes to .loc or [] with any missing label will raise\n",
            "KeyError in the future, you can use .reindex() as an alternative.\n",
            "\n",
            "See the documentation here:\n",
            "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDd0yYBBBzeO",
        "colab_type": "text"
      },
      "source": [
        "Now we make arrays of the desired inputs and outputs. The inputs are the embeddings, and the outputs are `1` for positive words and `-1` for negative words. We also make sure to keep track of the words they’re labeled with, so we can interpret the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-29T15:05:03.485323Z",
          "start_time": "2019-03-29T15:05:03.466320Z"
        },
        "id": "TPjdI-oKBzeP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectors = pd.concat([pos_vectors, neg_vectors])\n",
        "targets = np.array([1 for entry in pos_vectors.index] + [-1 for entry in neg_vectors.index])\n",
        "labels = list(pos_vectors.index) + list(neg_vectors.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-29T15:05:05.256478Z",
          "start_time": "2019-03-29T15:05:05.210348Z"
        },
        "id": "nBW36XlcBzeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_vectors, test_vectors, train_targets, test_targets, train_labels, test_labels = \\\n",
        "    train_test_split(vectors, targets, labels, test_size=0.1, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmOaXxGpBzeU",
        "colab_type": "text"
      },
      "source": [
        "Now it is time to make your classifier, and train it by running the training vectors through it for 100 iterations. You can use a logistic function as the loss, so that the resulting classifier can output the probability that a word is positive or negative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-29T15:16:50.546163Z",
          "start_time": "2019-03-29T15:16:50.542669Z"
        },
        "id": "LreSnh1RBzeU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "61d58295-1a53-43be-924a-5bf197f73300"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "model = SGDClassifier(loss='log', random_state=0, n_iter=100)\n",
        "model.fit(train_vectors, train_targets)\n",
        "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
        "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
        "       learning_rate='optimal', loss='log', n_iter=100, n_jobs=1,\n",
        "       penalty='l2', power_t=0.5, random_state=0, shuffle=True, verbose=0,\n",
        "       warm_start=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
              "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
              "       l1_ratio=0.15, learning_rate='optimal', loss='log', max_iter=None,\n",
              "       n_iter=100, n_iter_no_change=5, n_jobs=1, penalty='l2', power_t=0.5,\n",
              "       random_state=0, shuffle=True, tol=None, validation_fraction=0.1,\n",
              "       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UlxWk8NBzeX",
        "colab_type": "text"
      },
      "source": [
        "We can evaluate the classifier on the test vectors. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-29T15:16:54.627721Z",
          "start_time": "2019-03-29T15:16:54.625191Z"
        },
        "id": "gh0kOsUdBzeX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "ee0481e0-8595-44f5-d167-78d632d00e77"
      },
      "source": [
        "accuracy_score(model.predict(test_vectors), test_targets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9502262443438914"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "983emjCeBzeZ",
        "colab_type": "text"
      },
      "source": [
        "Let’s define a function that we can use to see the sentiment that this classifier predicts for particular words, then use it to see some examples of its predictions on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-29T15:06:07.705034Z",
          "start_time": "2019-03-29T15:06:07.670604Z"
        },
        "id": "QAtTmjPkBzea",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 736
        },
        "outputId": "47d024c5-1827-477f-a5a8-471499ffc0c6"
      },
      "source": [
        "def vecs_to_sentiment(vecs):\n",
        "    # predict_log_proba gives the log probability for each class\n",
        "    predictions = model.predict_log_proba(vecs)\n",
        "\n",
        "    # To see an overall positive vs. negative classification in one number,\n",
        "    # we take the log probability of positive sentiment minus the log\n",
        "    # probability of negative sentiment.\n",
        "    return predictions[:, 1] - predictions[:, 0]\n",
        "\n",
        "\n",
        "def words_to_sentiment(words):\n",
        "    vecs = embeddings.loc[words].dropna()\n",
        "    log_odds = vecs_to_sentiment(vecs)\n",
        "    return pd.DataFrame({'sentiment': log_odds}, index=vecs.index)\n",
        "\n",
        "\n",
        "# Show 20 examples from the test set\n",
        "words_to_sentiment(test_labels).ix[:20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: \n",
            ".ix is deprecated. Please use\n",
            ".loc for label based indexing or\n",
            ".iloc for positional indexing\n",
            "\n",
            "See the documentation here:\n",
            "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>fidget</th>\n",
              "      <td>-9.931679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>interrupt</th>\n",
              "      <td>-9.634706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>staunchly</th>\n",
              "      <td>1.466919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imaginary</th>\n",
              "      <td>-2.989215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>taxing</th>\n",
              "      <td>0.468522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>world-famous</th>\n",
              "      <td>6.908561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>low-cost</th>\n",
              "      <td>9.237223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disapointment</th>\n",
              "      <td>-8.737182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>totalitarian</th>\n",
              "      <td>-10.851580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bellicose</th>\n",
              "      <td>-8.328674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freezes</th>\n",
              "      <td>-8.456981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sin</th>\n",
              "      <td>-7.839670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fragile</th>\n",
              "      <td>-4.018289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fooled</th>\n",
              "      <td>-4.309344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>undecided</th>\n",
              "      <td>-2.816172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>handily</th>\n",
              "      <td>2.339609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>demonizes</th>\n",
              "      <td>-2.102152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>easygoing</th>\n",
              "      <td>8.747150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unpopular</th>\n",
              "      <td>-7.887475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>commiserate</th>\n",
              "      <td>1.790899</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               sentiment\n",
              "fidget         -9.931679\n",
              "interrupt      -9.634706\n",
              "staunchly       1.466919\n",
              "imaginary      -2.989215\n",
              "taxing          0.468522\n",
              "world-famous    6.908561\n",
              "low-cost        9.237223\n",
              "disapointment  -8.737182\n",
              "totalitarian  -10.851580\n",
              "bellicose      -8.328674\n",
              "freezes        -8.456981\n",
              "sin            -7.839670\n",
              "fragile        -4.018289\n",
              "fooled         -4.309344\n",
              "undecided      -2.816172\n",
              "handily         2.339609\n",
              "demonizes      -2.102152\n",
              "easygoing       8.747150\n",
              "unpopular      -7.887475\n",
              "commiserate     1.790899"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7CPVesRBzed",
        "colab_type": "text"
      },
      "source": [
        "## Sentiment score for text\n",
        "\n",
        "There are many ways to combine sentiments for word vectors into an overall sentiment score. The simplest way is to average them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-29T15:07:29.746290Z",
          "start_time": "2019-03-29T15:07:29.740768Z"
        },
        "id": "-XJpCFwjBzee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "TOKEN_RE = re.compile(r\"\\w.*?\\b\")\n",
        "# The regex above finds tokens that start with a word-like character (\\w), and continues\n",
        "# matching characters (.+?) until the next word break (\\b). It's a relatively simple\n",
        "# expression that manages to extract something very much like words from text.\n",
        "\n",
        "\n",
        "def text_to_sentiment(text):\n",
        "    tokens = [token.casefold() for token in TOKEN_RE.findall(text)]\n",
        "    sentiments = words_to_sentiment(tokens)\n",
        "    return sentiments['sentiment'].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-29T15:07:40.366537Z",
          "start_time": "2019-03-29T15:07:40.351478Z"
        },
        "id": "f_qHy19xBzeg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "6fb172e8-8e8d-452d-f392-57101961166e"
      },
      "source": [
        "text_to_sentiment(\"this example is pretty cool\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.889968926086298"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-29T15:07:48.548136Z",
          "start_time": "2019-03-29T15:07:48.540902Z"
        },
        "id": "1eJusL1qBzek",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "6cda0b49-53fe-4d91-ad50-717ff0b26bb5"
      },
      "source": [
        "text_to_sentiment(\"meh, this example sucks\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1.1774475917460698"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVSJqoSBBzep",
        "colab_type": "text"
      },
      "source": [
        "Let’s see what it does with a few variations on a neutral sentence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-29T15:08:39.114595Z",
          "start_time": "2019-03-29T15:08:39.099228Z"
        },
        "id": "E-oqx5yFBzeq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "outputId": "34f8a474-4ac2-4111-b432-ef089fc749b1"
      },
      "source": [
        "print(text_to_sentiment(\"Let's go get Italian food\"))\n",
        "print(text_to_sentiment(\"Let's go get Chinese food\"))\n",
        "print(text_to_sentiment(\"Let's go get Indian food\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0429166109408983\n",
            "1.4094033658140972\n",
            "1.5702502107020269\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-29T15:09:13.624994Z",
          "start_time": "2019-03-29T15:09:13.604583Z"
        },
        "id": "ojy7OzeyBzet",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "5f151301-023f-455e-d002-1941b8710d48"
      },
      "source": [
        "print(text_to_sentiment(\"My name is Emily\"))\n",
        "print(text_to_sentiment(\"My name is Heather\"))\n",
        "print(text_to_sentiment(\"My name is Yvette\"))\n",
        "print(text_to_sentiment(\"My name is Shaniqua\"))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.228617936474531\n",
            "1.3976291151079159\n",
            "0.9846380213298556\n",
            "-0.47048131775890656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMh897d-Bzew",
        "colab_type": "text"
      },
      "source": [
        "The system has associated wildly different sentiments with people’s names. You can look at these examples and many others and see that the sentiment is generally more positive for stereotypically-white names, and more negative for stereotypically-black names."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJLdr7ROBzew",
        "colab_type": "text"
      },
      "source": [
        "## Ethical problem\n",
        "\n",
        "We want to learn how to not make something like this again. So let’s put more data through it, and statistically measure how bad its bias is.\n",
        "\n",
        "Here we have four lists of names that tend to reflect different ethnic backgrounds, mostly from a United States perspective. The first two are lists of predominantly “white” and “black” names adapted from Caliskan et al.’s article. I also added typically Hispanic names, as well as Muslim names that come from Arabic or Urdu; these are two more distinct groupings of given names that tend to represent your background."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-29T15:10:18.816997Z",
          "start_time": "2019-03-29T15:10:18.787085Z"
        },
        "id": "eSAQ22POBzex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NAMES_BY_ETHNICITY = {\n",
        "    # The first two lists are from the Caliskan et al. appendix describing the\n",
        "    # Word Embedding Association Test.\n",
        "    'White': [\n",
        "        'Adam', 'Chip', 'Harry', 'Josh', 'Roger', 'Alan', 'Frank', 'Ian', 'Justin',\n",
        "        'Ryan', 'Andrew', 'Fred', 'Jack', 'Matthew', 'Stephen', 'Brad', 'Greg', 'Jed',\n",
        "        'Paul', 'Todd', 'Brandon', 'Hank', 'Jonathan', 'Peter', 'Wilbur', 'Amanda',\n",
        "        'Courtney', 'Heather', 'Melanie', 'Sara', 'Amber', 'Crystal', 'Katie',\n",
        "        'Meredith', 'Shannon', 'Betsy', 'Donna', 'Kristin', 'Nancy', 'Stephanie',\n",
        "        'Bobbie-Sue', 'Ellen', 'Lauren', 'Peggy', 'Sue-Ellen', 'Colleen', 'Emily',\n",
        "        'Megan', 'Rachel', 'Wendy'\n",
        "    ],\n",
        "\n",
        "    'Black': [\n",
        "        'Alonzo', 'Jamel', 'Lerone', 'Percell', 'Theo', 'Alphonse', 'Jerome',\n",
        "        'Leroy', 'Rasaan', 'Torrance', 'Darnell', 'Lamar', 'Lionel', 'Rashaun',\n",
        "        'Tyree', 'Deion', 'Lamont', 'Malik', 'Terrence', 'Tyrone', 'Everol',\n",
        "        'Lavon', 'Marcellus', 'Terryl', 'Wardell', 'Aiesha', 'Lashelle', 'Nichelle',\n",
        "        'Shereen', 'Temeka', 'Ebony', 'Latisha', 'Shaniqua', 'Tameisha', 'Teretha',\n",
        "        'Jasmine', 'Latonya', 'Shanise', 'Tanisha', 'Tia', 'Lakisha', 'Latoya',\n",
        "        'Sharise', 'Tashika', 'Yolanda', 'Lashandra', 'Malika', 'Shavonn',\n",
        "        'Tawanda', 'Yvette'\n",
        "    ],\n",
        "    \n",
        "    # This list comes from statistics about common Hispanic-origin names in the US.\n",
        "    'Hispanic': [\n",
        "        'Juan', 'José', 'Miguel', 'Luís', 'Jorge', 'Santiago', 'Matías', 'Sebastián',\n",
        "        'Mateo', 'Nicolás', 'Alejandro', 'Samuel', 'Diego', 'Daniel', 'Tomás',\n",
        "        'Juana', 'Ana', 'Luisa', 'María', 'Elena', 'Sofía', 'Isabella', 'Valentina',\n",
        "        'Camila', 'Valeria', 'Ximena', 'Luciana', 'Mariana', 'Victoria', 'Martina'\n",
        "    ],\n",
        "    \n",
        "    # The following list conflates religion and ethnicity, I'm aware. So do given names.\n",
        "    #\n",
        "    # This list was cobbled together from searching baby-name sites for common Muslim names,\n",
        "    # as spelled in English. I did not ultimately distinguish whether the origin of the name\n",
        "    # is Arabic or Urdu or another language.\n",
        "    #\n",
        "    # I'd be happy to replace it with something more authoritative, given a source.\n",
        "    'Arab/Muslim': [\n",
        "        'Mohammed', 'Omar', 'Ahmed', 'Ali', 'Youssef', 'Abdullah', 'Yasin', 'Hamza',\n",
        "        'Ayaan', 'Syed', 'Rishaan', 'Samar', 'Ahmad', 'Zikri', 'Rayyan', 'Mariam',\n",
        "        'Jana', 'Malak', 'Salma', 'Nour', 'Lian', 'Fatima', 'Ayesha', 'Zahra', 'Sana',\n",
        "        'Zara', 'Alya', 'Shaista', 'Zoya', 'Yasmin'\n",
        "    ]\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8B5G2exKBze1",
        "colab_type": "text"
      },
      "source": [
        "Now we’ll use Pandas to make a table of these names, their predominant ethnic background, and the sentiment score we get for them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-29T15:10:46.744680Z",
          "start_time": "2019-03-29T15:10:46.676412Z"
        },
        "id": "mgePQtf6Bze1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "outputId": "77b14e65-660b-48b4-94e7-f7175a5582fb"
      },
      "source": [
        "def name_sentiment_table():\n",
        "    frames = []\n",
        "    for group, name_list in sorted(NAMES_BY_ETHNICITY.items()):\n",
        "        lower_names = [name.lower() for name in name_list]\n",
        "        sentiments = words_to_sentiment(lower_names)\n",
        "        sentiments['group'] = group\n",
        "        frames.append(sentiments)\n",
        "\n",
        "    # Put together the data we got from each ethnic group into one big table\n",
        "    return pd.concat(frames)\n",
        "\n",
        "name_sentiments = name_sentiment_table()\n",
        "\n",
        "name_sentiments.ix[::25]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: FutureWarning: \n",
            "Passing list-likes to .loc or [] with any missing label will raise\n",
            "KeyError in the future, you can use .reindex() as an alternative.\n",
            "\n",
            "See the documentation here:\n",
            "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: FutureWarning: \n",
            "Passing list-likes to .loc or [] with any missing label will raise\n",
            "KeyError in the future, you can use .reindex() as an alternative.\n",
            "\n",
            "See the documentation here:\n",
            "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: FutureWarning: \n",
            "Passing list-likes to .loc or [] with any missing label will raise\n",
            "KeyError in the future, you can use .reindex() as an alternative.\n",
            "\n",
            "See the documentation here:\n",
            "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: \n",
            ".ix is deprecated. Please use\n",
            ".loc for label based indexing or\n",
            ".iloc for positional indexing\n",
            "\n",
            "See the documentation here:\n",
            "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>group</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>mohammed</th>\n",
              "      <td>0.834974</td>\n",
              "      <td>Arab/Muslim</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>alya</th>\n",
              "      <td>3.916803</td>\n",
              "      <td>Arab/Muslim</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>terryl</th>\n",
              "      <td>-2.858010</td>\n",
              "      <td>Black</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>josé</th>\n",
              "      <td>0.432956</td>\n",
              "      <td>Hispanic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>luciana</th>\n",
              "      <td>1.086073</td>\n",
              "      <td>Hispanic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hank</th>\n",
              "      <td>0.391858</td>\n",
              "      <td>White</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>megan</th>\n",
              "      <td>2.158679</td>\n",
              "      <td>White</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          sentiment        group\n",
              "mohammed   0.834974  Arab/Muslim\n",
              "alya       3.916803  Arab/Muslim\n",
              "terryl    -2.858010        Black\n",
              "josé       0.432956     Hispanic\n",
              "luciana    1.086073     Hispanic\n",
              "hank       0.391858        White\n",
              "megan      2.158679        White"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJA6-N1KXKNQ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-29T15:10:59.406581Z",
          "start_time": "2019-03-29T15:10:59.243215Z"
        },
        "id": "UmGNsOjdBze4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "4e9131d2-c255-428c-c619-06aaf10bbc6e"
      },
      "source": [
        "import seaborn\n",
        "\n",
        "plot = seaborn.swarmplot(x='group', y='sentiment', data=name_sentiments)\n",
        "plot.set_ylim([-10, 10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-10, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEKCAYAAAA1qaOTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmZXCAcXMzErhgGJmZqX4P5hC7F+xFpH6AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-29T15:11:09.382514Z",
          "start_time": "2019-03-29T15:11:09.222382Z"
        },
        "id": "C_C11E9xBze6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "526aaead-c022-4bd1-94c1-4a341c39291f"
      },
      "source": [
        "plot = seaborn.barplot(x='group', y='sentiment', data=name_sentiments, capsize=.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEKCAYAAAASByJ7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nECRJAPx/3zi6T20JMBQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kmvb2yKmVXkI",
        "colab_type": "text"
      },
      "source": [
        "# Exercise\n",
        "\n",
        "We have used only word labels to build a sentiment classifier, but if we have sentence labels we can build a model that uses both, word and sentence information.\n",
        "\n",
        "To this end, you can use the `imdb` dataset (available in Keras): https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py \n",
        "\n",
        "Yours tasks are:\n",
        "+ Use the sentiment model we have already developed (based on words), to evaluate the `imdb` dataset. What is the accuracy of this classifier?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9IzpWCMaADC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install numpy==1.16.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNBmoprbmyE5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing the data and spliting it into train and test set, determining max features\n",
        "from keras.datasets import imdb\n",
        "import pandas as pd\n",
        "\n",
        "max_features = 20000\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRvKJsGTJN7e",
        "colab_type": "text"
      },
      "source": [
        "Treshold was determined by experimenting - The model or the data are probably biased towards positive words, so only if a word gets sentiment score of 1.55 or higher it's classified as positive (1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vguxyX2Zz4Uo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = dict(\n",
        "[(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def sequence_to_text(sequence):\n",
        "    decoded_review = ' '.join(\n",
        "    [reverse_word_index.get(i - 3, '?') for i in sequence])\n",
        "    return decoded_review\n",
        "\n",
        "def get_sentiment(sequence):\n",
        "    score = text_to_sentiment(sequence_to_text(sequence))\n",
        "    if score >= 1.55:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JZt2uJ-Dp9o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "27c8942f-afc7-4724-fd2b-33c34ce5012f"
      },
      "source": [
        "#quick check\n",
        "print(text_to_sentiment('Titanic'))\n",
        "print(get_sentiment(x_train[15]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5424886970379013\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogCsFowc03rE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "616a08e1-d652-44e1-c3ce-17be4e0550cb"
      },
      "source": [
        "import numpy as np\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "y_pred = np.array([get_sentiment(i) for i in x_train[:25000]])\n",
        "accuracy_score(y_train[:25000], y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.75232"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzD_Wrsf4Fwd",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "+ Train a LSTM model to classify sentences based on the `imdb` dataset but, insteasd of learning specific embeddings for this problem, use `GloVe`.  What is the accuracy of this classifier?\n",
        "\n",
        "(Changing the embedding to glove, getting the weight matrix and getting the embedding vectors for further analysis.)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STStDDtmZNS2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "ef622ddd-e768-4264-ee5f-8cca10e69be7"
      },
      "source": [
        "def load_embedding(filename):\n",
        "\t# load embedding into memory, skip first line\n",
        "\tfile = open(filename,'r')\n",
        "\tlines = file.readlines()[1:]\n",
        "\tfile.close()\n",
        "\t# create a map of words to vectors\n",
        "\tembedding = dict()\n",
        "\tfor line in lines:\n",
        "\t\tparts = line.split()\n",
        "\t\t# key is string word, value is numpy array for vector\n",
        "\t\tembedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
        "\treturn embedding\n",
        "\n",
        "embeddings = load_embeddings('glove.42B.300d.txt')\n",
        "embeddings.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1917494it [03:50, 8328.58it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1917494, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XKkLGWkppnM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_weight_matrix(embedding, vocab):\n",
        "    # total vocabulary size plus 0 for unknown words\n",
        "    vocab_size = len(vocab) + 1\n",
        "    # define weight matrix dimensions with all 0\n",
        "    weight_matrix = np.zeros((vocab_size, 100))\n",
        "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
        "    for word, i in vocab.items():\n",
        "        vector = embedding.get(word)\n",
        "        if vector is not None:\n",
        "            weight_matrix[i] = vector\n",
        "    return weight_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsTDWu0yCDjz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4145
        },
        "outputId": "9f7589c0-1131-4764-a72a-aebc481d93f8"
      },
      "source": [
        "embedding_vectors = embeddings[:max_features]\n",
        "print(embedding_vectors.shape)\n",
        "#print(embedding_vectors)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20000, 300)\n",
            "                    0         1         2         3         4         5    \\\n",
            ",              0.183780 -0.121230 -0.119870  0.015227 -0.191210 -0.066074   \n",
            "the           -0.208380 -0.149320 -0.017528 -0.028432 -0.060104 -0.264600   \n",
            ".              0.108760  0.002244  0.222130 -0.121020 -0.048959  0.018135   \n",
            "and           -0.096110 -0.257880 -0.358600 -0.328870  0.579500 -0.517740   \n",
            "to            -0.248370 -0.454610  0.039227 -0.284220 -0.031852  0.263550   \n",
            "of            -0.036429 -0.285920  0.063387 -0.601220 -0.015309  0.073243   \n",
            "a             -0.035835  0.778440 -0.518060  0.080682 -0.131730 -0.286060   \n",
            "in             0.068507 -0.023344  0.282710 -0.402150  0.077815 -0.027003   \n",
            "\"             -0.048032 -0.691400 -0.219570  0.089568  0.284720 -1.053100   \n",
            "is             0.113960  0.345030 -0.055064 -0.201920  0.208480 -0.096990   \n",
            "for           -0.239090 -0.641890 -0.583220 -0.547430  0.423860 -0.117550   \n",
            ":              0.185240 -0.378870  0.266030 -0.593010  0.655070 -0.088332   \n",
            "i             -0.043504 -0.184840 -0.146130 -0.217510  0.202500  0.044053   \n",
            ")              0.266670 -0.230150 -0.013520  0.109430  0.711970 -0.090025   \n",
            "that           0.158050 -0.262330  0.174180 -0.168790  0.132490  0.001213   \n",
            "(              0.225660 -0.371120 -0.215450 -0.016410  0.330530 -0.152350   \n",
            "you           -0.045315 -0.010389 -0.197980  0.007407  0.139150  0.283150   \n",
            "it             0.394570 -0.013387 -0.232890 -0.160100  0.327880 -0.071888   \n",
            "on             0.000607  0.048631  0.489690  0.427770 -0.386100 -0.008423   \n",
            "-             -0.278810 -0.402710 -0.185910 -0.272020  0.284090  0.253640   \n",
            "with          -0.505900 -0.548400  0.016064 -0.295800 -0.011176 -0.342650   \n",
            "'s             0.487010 -0.263730 -0.306250  0.146840  0.203050 -0.136410   \n",
            "this           0.192230  0.285540  0.012629 -0.228840  0.940520 -0.362770   \n",
            "by            -0.115760 -0.060881 -0.134010 -0.614350 -0.197550  0.234180   \n",
            "are           -0.321440  0.115290  0.009374 -0.760130 -0.026915  0.237430   \n",
            "at             0.198580 -0.212950  0.581360  0.117030 -0.359020 -0.297980   \n",
            "as             0.083738  0.134010 -0.406470 -0.349610  0.076245 -0.372060   \n",
            "be            -0.244430  0.033343  0.062834 -0.325830  0.241490  0.262660   \n",
            "from          -0.360800  0.253370 -0.159060 -0.619260  0.370830 -0.440160   \n",
            "have          -0.170510  0.085940 -0.007724 -0.202030  0.056731  0.069374   \n",
            "...                 ...       ...       ...       ...       ...       ...   \n",
            "juin          -0.075638 -0.034237 -0.508660  0.238320  0.561780  0.202110   \n",
            "??????        -0.447400  0.123290  0.126830  0.176960  0.483460  0.360470   \n",
            "envision      -0.443760  0.104210  0.001530 -0.300340 -0.297220  0.004622   \n",
            ",2            -0.001362  0.142950 -0.110190  0.282910 -0.064431  0.342910   \n",
            "marched       -0.044126  0.226040  0.289140 -0.561490 -0.859620 -0.564180   \n",
            "alchemy       -0.263600 -0.421780  0.338160 -0.204930  0.048430  0.627840   \n",
            "kjv           -0.163530 -0.944020  0.109900 -0.494390 -0.486110  0.666560   \n",
            "gangbang      -0.376500 -0.024094  0.027268  0.473260 -0.002410  0.199550   \n",
            "abyss         -0.632800 -0.444490  0.131080 -0.271070  0.164940  0.069235   \n",
            "gould          0.031923  0.009283 -0.186950 -0.066530 -0.770560 -0.284230   \n",
            "1pm            0.187160 -0.268300  0.451270 -0.024859  0.060309  0.408650   \n",
            "heed          -0.341030  0.349530  0.748420  0.113300 -0.715110  0.493420   \n",
            "refrigerators -0.017866  0.196910 -0.220490 -0.163420 -0.148420  0.558670   \n",
            "obligated      0.041143 -0.411870 -0.221300  0.497740 -0.557900  0.752750   \n",
            "opus          -0.036992 -0.484830  0.328270 -0.038435 -0.013421  0.244040   \n",
            "hoc            0.620050 -0.357580  0.166650  0.146080  0.053640  0.228450   \n",
            "lumbar         0.597020  0.139390  0.004732 -0.548310  0.541620 -0.118240   \n",
            "benefiting     0.026328  0.020496  0.028947 -0.395600 -0.047566  0.382410   \n",
            "urgently       0.264770  0.254630  0.576720  0.101430  0.280870  0.149260   \n",
            "shovel        -0.195990  0.523600  0.613910 -0.767710 -0.295230 -0.327440   \n",
            "heightened    -0.099736 -0.011920  0.463230 -0.671180 -0.075266 -0.387070   \n",
            "blasted       -0.320570  0.085763  0.250700 -0.126280 -0.144250  0.048366   \n",
            "outskirts      0.532680  0.347440  0.190410 -0.158940 -0.624600  0.025853   \n",
            "listprice     -0.156560  0.434350 -0.937350 -0.271550  0.492060  0.366820   \n",
            "ethiopian      0.440870  0.071781 -0.221890 -0.428660 -0.734320  0.393960   \n",
            "handgun        0.329820  0.500330  0.117450 -0.503120  0.638400  0.382670   \n",
            "numeric       -0.002770  0.245270 -0.063236  0.152200 -0.058920  0.402070   \n",
            "saas           0.999040 -0.944010 -0.200950 -0.767280  0.457420 -0.473930   \n",
            "discerning    -0.237740 -0.080310 -0.223090 -0.051473  0.044399  0.429260   \n",
            "imbalance      0.032425 -0.055697  0.142750 -0.121030 -0.327740 -0.086968   \n",
            "\n",
            "                    6         7         8         9    ...       290  \\\n",
            ",             -2.987600  0.807950  0.067338 -0.131840  ...  0.136150   \n",
            "the           -4.144500  0.629320  0.336720 -0.433950  ... -0.041244   \n",
            ".             -3.817400 -0.032631 -0.625940 -0.518980  ...  0.063131   \n",
            "and           -4.158200 -0.113710 -0.108480 -0.488850  ...  0.477810   \n",
            "to            -4.632300  0.013890 -0.539280 -0.084454  ...  0.082736   \n",
            "of            -3.854700  0.528090 -0.077859 -0.407560  ... -0.021308   \n",
            "a             -4.248500  0.818270  0.240340 -0.690570  ...  0.084727   \n",
            "in            -3.764500  0.417080  0.058373 -0.067997  ...  0.095986   \n",
            "\"             -3.412100  0.026960 -0.200370 -0.530910  ...  0.196990   \n",
            "is            -4.023600  0.950460 -0.060803 -0.832600  ... -0.085202   \n",
            "for           -4.216900  0.286850 -0.307660 -0.674200  ...  0.241210   \n",
            ":             -2.511000  0.779130 -0.602890 -0.428770  ...  0.775330   \n",
            "i             -4.282300 -0.034931  0.103370 -0.689310  ... -0.152220   \n",
            ")             -2.576800  0.886590  0.210250 -0.319480  ...  0.021150   \n",
            "that          -4.776200  0.292780  0.064338 -0.574040  ...  0.149730   \n",
            "(             -2.706400  0.939840  0.205240  0.143410  ...  0.250210   \n",
            "you           -4.857800  0.059722 -0.041928 -0.679510  ...  0.089044   \n",
            "it            -4.916600  0.420020 -0.070704 -0.723140  ... -0.022828   \n",
            "on            -3.602700  0.478110  0.047945 -0.318590  ...  0.271620   \n",
            "-             -2.552600  0.129130 -0.597990 -0.663680  ...  0.425410   \n",
            "with          -4.186200  0.270890 -0.125630 -0.873790  ... -0.034727   \n",
            "'s            -3.537600  0.592380  0.216570 -1.411300  ...  0.101090   \n",
            "this          -4.309800  0.328530 -0.599500 -0.577310  ...  0.105860   \n",
            "by            -3.375400 -0.108720  0.516460 -0.093283  ... -0.038541   \n",
            "are           -4.317100  0.026367  0.279940 -0.517300  ... -0.194300   \n",
            "at            -3.312800 -0.408370  0.548420  0.011263  ... -0.278140   \n",
            "as            -4.242800  0.516060 -0.108430 -0.439180  ...  0.092935   \n",
            "be            -4.515700  0.752120  0.275280 -0.459580  ... -0.514880   \n",
            "from          -3.658900 -0.448550 -0.758400  0.158620  ...  0.244580   \n",
            "have          -4.359500  0.334720  0.260200 -0.662850  ... -0.203520   \n",
            "...                 ...       ...       ...       ...  ...       ...   \n",
            "juin          -0.242210  0.193720  0.576140  0.450520  ...  0.378620   \n",
            "??????        -0.736690  0.103290 -0.047730  0.142280  ... -0.043584   \n",
            "envision      -1.710800 -0.233470  0.469820 -0.037402  ... -0.141990   \n",
            ",2            -0.432370  0.513130 -0.024124  0.414110  ...  0.193690   \n",
            "marched       -0.631470 -0.424120  0.410910  0.436870  ... -0.570210   \n",
            "alchemy       -1.024900 -0.321750  0.225760 -0.219270  ...  0.270740   \n",
            "kjv           -0.661380  0.182950 -0.674890 -0.212600  ...  0.122000   \n",
            "gangbang      -0.463130 -0.021004 -0.437660 -0.973220  ... -0.031552   \n",
            "abyss         -1.230000  0.102300 -0.194590 -0.172170  ... -0.211950   \n",
            "gould         -0.122110 -0.554070 -0.095506 -0.214530  ... -0.131440   \n",
            "1pm           -0.566690 -0.673790  0.337270  0.251430  ...  0.394080   \n",
            "heed          -1.827200 -0.015902  0.343210 -0.026681  ...  0.129470   \n",
            "refrigerators -0.890290 -0.454730  0.047207  0.287600  ...  0.513910   \n",
            "obligated     -1.979700  0.223480 -0.093299 -0.007432  ... -0.195770   \n",
            "opus          -0.639380 -0.596310 -0.054096  0.405900  ... -0.556370   \n",
            "hoc           -1.210400  0.801160  0.210300  0.968740  ... -0.254950   \n",
            "lumbar        -0.843740  0.851940 -0.388760  0.191360  ...  0.074863   \n",
            "benefiting    -1.654400 -0.818460 -0.243850  0.067342  ... -0.238760   \n",
            "urgently      -1.998200 -0.076942 -0.106450  0.279970  ... -0.675660   \n",
            "shovel        -0.898030  0.713590  0.109870 -0.290910  ...  0.235030   \n",
            "heightened    -2.163600  0.258380  0.150060 -0.173150  ... -0.076387   \n",
            "blasted       -1.134900 -0.116820 -0.037700 -0.221890  ...  0.090773   \n",
            "outskirts     -0.815980 -0.445890 -0.360810  0.305960  ... -0.421020   \n",
            "listprice      0.080094 -0.538080 -0.367530  0.336000  ...  0.319220   \n",
            "ethiopian     -0.674070 -0.186220 -0.205080  0.143480  ... -0.449220   \n",
            "handgun       -1.090500 -0.003227 -0.611730 -0.067100  ... -0.386680   \n",
            "numeric       -1.496000  1.040100  0.252290 -0.421930  ...  0.370080   \n",
            "saas          -1.656500  0.229290  0.482370  0.458210  ... -0.084584   \n",
            "discerning    -1.928200 -0.574180  0.559410  0.248290  ...  0.408670   \n",
            "imbalance     -1.781400  0.937910 -0.180300 -0.203510  ...  0.011169   \n",
            "\n",
            "                    291       292       293       294       295       296  \\\n",
            ",              0.109990 -0.334740 -0.046109  0.107800 -0.035657 -0.012921   \n",
            "the           -0.461820  0.027903  0.546570 -0.258940  0.395150  0.261440   \n",
            ".             -0.212300 -0.300880 -0.451610  0.264800  0.075971 -0.406880   \n",
            "and           -0.021213 -0.212360  0.423740  0.140830  0.067498 -0.126750   \n",
            "to            -0.624690  0.044267  0.606730 -0.124580 -0.154430 -0.163390   \n",
            "of            -0.660190  0.090383  0.128590  0.226160  0.302350 -0.257030   \n",
            "a              0.104590 -0.253590 -0.120020 -0.389650 -0.287800  0.003670   \n",
            "in            -0.670370  0.599370 -0.146990  0.116970 -0.356280  0.112240   \n",
            "\"              0.260160 -0.598070  0.477600  0.489110 -0.338950 -0.430470   \n",
            "is            -0.018587 -0.258090  0.217390  0.215920  0.081633  0.122240   \n",
            "for           -0.261480  0.051920  0.035589 -0.155700  0.293410 -0.090526   \n",
            ":              1.247200 -0.911700 -0.126740  0.392120 -0.282310 -0.873580   \n",
            "i             -0.101190 -0.205080 -0.375180  0.216230  0.264670 -0.081656   \n",
            ")              0.083233 -0.776120 -0.124640  0.135370  0.190150  0.124500   \n",
            "that           0.002091 -0.162670 -0.012839  0.208860 -0.228570  0.226020   \n",
            "(             -0.227680 -0.645480  0.099261  0.081619 -0.208600 -0.333460   \n",
            "you           -0.184120 -0.360550 -0.120080  0.119200 -0.108770 -0.194210   \n",
            "it            -0.005892 -0.221670 -0.130030  0.158240  0.041910  0.360910   \n",
            "on             0.271120 -0.275490  0.285450  0.149250 -0.280460 -0.494460   \n",
            "-              0.243650  0.046829 -0.418520  0.776650  0.378930 -0.007552   \n",
            "with          -0.082055 -0.226320 -0.103620 -0.080037  0.011467 -0.440090   \n",
            "'s            -0.271220 -0.549860 -0.047373  0.036111  0.007525  0.457020   \n",
            "this           0.481860  0.032621 -0.186410 -0.104600 -0.306770 -0.300530   \n",
            "by            -0.344150 -0.439790 -0.486820 -0.121200  0.214940  0.400480   \n",
            "are           -0.168220 -0.111740 -0.278210  0.136520 -0.067655 -0.128240   \n",
            "at            -0.042609 -0.226710  0.498750 -0.294220  0.579100 -0.264900   \n",
            "as            -0.181430 -0.275440  0.095794  0.233040 -0.021151 -0.267060   \n",
            "be            -0.227350 -0.346350 -0.076397  0.173120  0.013200  0.230740   \n",
            "from           0.610580  0.092489 -0.326860 -0.374370  0.269270  0.001546   \n",
            "have          -0.089551 -0.030590 -0.455410 -0.039690 -0.331230 -0.115860   \n",
            "...                 ...       ...       ...       ...       ...       ...   \n",
            "juin           0.232270  0.256090  0.190290  0.397850  0.203900 -0.335310   \n",
            "??????        -0.406250 -0.470640  0.090315 -0.392570  0.004215 -0.524880   \n",
            "envision      -0.212740 -0.500460 -0.020263  0.444580  0.080521 -0.227800   \n",
            ",2             0.044445 -0.057067  0.728190  0.004271 -0.183040  0.036845   \n",
            "marched       -0.111930  0.346050  0.144780 -0.585180  1.000200 -0.251100   \n",
            "alchemy       -0.447830  0.245570 -0.067782  0.376870  0.254620 -0.657560   \n",
            "kjv           -0.369260 -0.283100  0.146250 -0.654450  0.851500 -0.417560   \n",
            "gangbang      -0.254900  0.639590  0.082024 -0.830850  0.241040  0.412340   \n",
            "abyss          0.687420  0.099302 -0.313100  0.069282  0.421590 -0.265610   \n",
            "gould         -0.204330 -0.032271  0.261120  0.149130  0.288750 -0.232070   \n",
            "1pm            0.010925  0.324090 -0.231980 -0.340220  1.053700  0.016121   \n",
            "heed          -0.241810 -0.385530  0.409210  0.101730  0.213560 -0.285750   \n",
            "refrigerators  0.381620 -0.121210 -0.127380  0.835830 -0.852870 -1.064500   \n",
            "obligated      0.153220 -0.606090 -0.091030  0.309070  0.332310  0.142580   \n",
            "opus          -0.164150 -0.245590 -0.432510 -0.257150  0.368660  0.435390   \n",
            "hoc            0.431960  0.153990  0.091650 -0.014866 -0.041897  0.153070   \n",
            "lumbar         0.055724  0.172470  0.107740 -0.484000 -0.683850 -0.011161   \n",
            "benefiting    -0.026707 -0.146630 -0.005119  0.256380  0.002170  0.032429   \n",
            "urgently      -0.355120 -1.172500 -0.265710  0.333960 -0.458470 -0.146610   \n",
            "shovel         0.025148  0.179560 -0.147130 -0.200890  0.136700  0.064690   \n",
            "heightened    -0.483300  0.144240 -0.178470  0.118150  0.130920  0.227310   \n",
            "blasted       -0.233780  0.118130 -0.013425  0.381690  0.126250  0.038706   \n",
            "outskirts     -0.071051 -0.363210 -0.109560 -0.155390  0.168940  0.500090   \n",
            "listprice      0.564930  0.167000 -0.661000  0.419690 -0.746180  0.333490   \n",
            "ethiopian      0.218380  0.036712  0.084702  0.058861 -0.545240 -0.021665   \n",
            "handgun       -0.398460  0.206390  0.103340 -0.449230  0.388670  0.233420   \n",
            "numeric       -0.288530 -0.158320  0.094051 -0.009556  0.440380 -0.179410   \n",
            "saas           0.871890  0.381620 -0.112010  0.317870 -0.062428 -0.159060   \n",
            "discerning    -0.933680 -0.711920 -0.205910  0.635190  0.080902  0.551340   \n",
            "imbalance     -0.061067  0.344110  0.496320  0.050242 -0.652280  0.248730   \n",
            "\n",
            "                    297       298       299  \n",
            ",             -0.039038  0.182740  0.146540  \n",
            "the           -0.540660  0.211990 -0.009436  \n",
            ".             -0.296960  0.159390 -0.149020  \n",
            "and           -0.370300 -0.092774  0.390580  \n",
            "to             0.053097  0.154580 -0.380530  \n",
            "of            -0.489260  0.305370  0.272730  \n",
            "a              0.022321 -0.315910 -0.356080  \n",
            "in            -0.234470  0.405500 -0.280670  \n",
            "\"             -0.087665 -0.229180 -0.004468  \n",
            "is            -0.372470  0.287900 -0.180290  \n",
            "for           -0.274330  0.019076  0.126920  \n",
            ":             -0.177600 -0.248930 -0.316300  \n",
            "i              0.100800  0.106800  0.089065  \n",
            ")             -0.316270 -0.086000 -0.701070  \n",
            "that          -0.317240  0.237490  0.311610  \n",
            "(             -0.373210 -0.121990 -0.839650  \n",
            "you           -0.414840  0.111240 -0.120420  \n",
            "it            -0.174510 -0.092674  0.201480  \n",
            "on             0.137310  0.138660  0.314000  \n",
            "-              0.378800  0.255320  0.315180  \n",
            "with           0.184810  0.196710  0.159020  \n",
            "'s            -0.189710  0.132280 -0.057165  \n",
            "this          -0.255770 -0.207130 -0.021695  \n",
            "by            -0.169470 -0.111430  0.243690  \n",
            "are           -0.287730  0.220030  0.195640  \n",
            "at             0.293370  0.194810 -0.320420  \n",
            "as            -0.196360 -0.115330  0.025416  \n",
            "be            -0.264270  0.100760 -0.332140  \n",
            "from           0.127490  0.154240 -0.290480  \n",
            "have          -0.232040  0.176050  0.215440  \n",
            "...                 ...       ...       ...  \n",
            "juin          -0.008293  0.626550  0.638390  \n",
            "??????        -0.237680  0.792250 -0.460130  \n",
            "envision       0.614060  0.753040 -0.449590  \n",
            ",2             0.414200  0.685810  0.288280  \n",
            "marched       -0.057961 -0.196430  0.085805  \n",
            "alchemy       -0.244110  0.433200 -0.068914  \n",
            "kjv            0.018546  0.462720 -0.380570  \n",
            "gangbang       0.092335 -0.099169 -0.180970  \n",
            "abyss          0.128420 -0.061969  0.050350  \n",
            "gould         -0.103230  0.386990  0.069096  \n",
            "1pm            0.090859 -0.544960 -0.208960  \n",
            "heed          -0.709200  0.115020  0.060169  \n",
            "refrigerators  0.029283 -0.160060 -0.361610  \n",
            "obligated      0.117030  0.353000 -0.415230  \n",
            "opus           0.417430  0.014383 -0.137990  \n",
            "hoc            0.432860 -0.784070 -0.214470  \n",
            "lumbar         0.401810  0.380730  0.613000  \n",
            "benefiting     0.720890  0.471100  0.242270  \n",
            "urgently       0.220150  0.165070 -0.257750  \n",
            "shovel         0.254810  0.045519  0.584270  \n",
            "heightened    -0.172000 -0.200130 -0.153260  \n",
            "blasted        0.026551 -0.311260  0.275940  \n",
            "outskirts     -0.126240 -0.293800 -0.474390  \n",
            "listprice     -0.882850  0.026019 -0.064520  \n",
            "ethiopian      0.186720 -0.523600 -0.667030  \n",
            "handgun       -0.332200 -0.461020 -0.192500  \n",
            "numeric       -0.227830 -0.094043  0.184930  \n",
            "saas           0.517490  0.257940  0.321650  \n",
            "discerning     0.025003  0.131090 -0.835710  \n",
            "imbalance     -0.562250  0.410760  0.484180  \n",
            "\n",
            "[20000 rows x 300 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tS064dil_gXz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 866
        },
        "outputId": "1a8cdfe8-a9e5-47c1-d7ee-62f7d6ea2b50"
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.datasets import imdb\n",
        "\n",
        "\n",
        "\n",
        "max_features = 20000\n",
        "maxlen = 80\n",
        "#batch_size = 32\n",
        "batch_size = 1000\n",
        "weights=embedding_vectors\n",
        "embedding_layer = Embedding(max_features, 300, weights=[embedding_vectors])\n",
        "\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(embedding_layer)\n",
        "model2.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model2.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "model2.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model2.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=15,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "score, acc = model2.evaluate(x_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)\n",
        "\n",
        "y_pred2 = model2.predict(y_test)\n",
        "\n",
        "\n",
        "#model.save('LSTM_model.h5')\n",
        "#from keras.models import load_model\n",
        "#modeL = load_model(LSTM_model.h5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "25000 train sequences\n",
            "25000 test sequences\n",
            "Pad sequences (samples x time)\n",
            "x_train shape: (25000, 80)\n",
            "x_test shape: (25000, 80)\n",
            "Build model...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Train...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/15\n",
            "25000/25000 [==============================] - 12s 470us/step - loss: 0.6768 - acc: 0.5650 - val_loss: 0.6450 - val_acc: 0.6214\n",
            "Epoch 2/15\n",
            "25000/25000 [==============================] - 10s 386us/step - loss: 0.5980 - acc: 0.6739 - val_loss: 0.5269 - val_acc: 0.7337\n",
            "Epoch 3/15\n",
            "25000/25000 [==============================] - 10s 385us/step - loss: 0.4917 - acc: 0.7598 - val_loss: 0.4330 - val_acc: 0.7992\n",
            "Epoch 4/15\n",
            "25000/25000 [==============================] - 10s 384us/step - loss: 0.4053 - acc: 0.8141 - val_loss: 0.3950 - val_acc: 0.8226\n",
            "Epoch 5/15\n",
            "25000/25000 [==============================] - 10s 385us/step - loss: 0.3393 - acc: 0.8546 - val_loss: 0.3851 - val_acc: 0.8336\n",
            "Epoch 6/15\n",
            "25000/25000 [==============================] - 10s 385us/step - loss: 0.2898 - acc: 0.8795 - val_loss: 0.3929 - val_acc: 0.8376\n",
            "Epoch 7/15\n",
            "25000/25000 [==============================] - 10s 383us/step - loss: 0.2481 - acc: 0.8998 - val_loss: 0.4037 - val_acc: 0.8370\n",
            "Epoch 8/15\n",
            "25000/25000 [==============================] - 9s 379us/step - loss: 0.2046 - acc: 0.9218 - val_loss: 0.4125 - val_acc: 0.8356\n",
            "Epoch 9/15\n",
            "25000/25000 [==============================] - 10s 385us/step - loss: 0.1726 - acc: 0.9362 - val_loss: 0.4365 - val_acc: 0.8343\n",
            "Epoch 10/15\n",
            "25000/25000 [==============================] - 10s 384us/step - loss: 0.1435 - acc: 0.9476 - val_loss: 0.4748 - val_acc: 0.8297\n",
            "Epoch 11/15\n",
            "25000/25000 [==============================] - 10s 385us/step - loss: 0.1148 - acc: 0.9606 - val_loss: 0.5251 - val_acc: 0.8243\n",
            "Epoch 12/15\n",
            "25000/25000 [==============================] - 10s 385us/step - loss: 0.0977 - acc: 0.9665 - val_loss: 0.5821 - val_acc: 0.8235\n",
            "Epoch 13/15\n",
            "25000/25000 [==============================] - 10s 385us/step - loss: 0.0795 - acc: 0.9739 - val_loss: 0.6808 - val_acc: 0.8151\n",
            "Epoch 14/15\n",
            "25000/25000 [==============================] - 10s 384us/step - loss: 0.0675 - acc: 0.9772 - val_loss: 0.7000 - val_acc: 0.8180\n",
            "Epoch 15/15\n",
            "25000/25000 [==============================] - 10s 384us/step - loss: 0.0542 - acc: 0.9812 - val_loss: 0.7245 - val_acc: 0.8166\n",
            "25000/25000 [==============================] - 2s 86us/step\n",
            "Test score: 0.7245154285430908\n",
            "Test accuracy: 0.8166400003433227\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdMhkeiM1hxZ",
        "colab_type": "text"
      },
      "source": [
        "##Hybrid model\n",
        "\n",
        "+ Build and train a hybrid model for taking into account word and sentence labels. What is the accuracy of this classifier when applied the `imdb` dataset ?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7WUS5qfjagE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#using predictions from the first model - y_pred and predictions from\n",
        "#the second model - y_pred2 and concatenating them together.\n",
        "X = np.c_[y_pred,y_pred2]\n",
        "y = y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARaqJt1-HYsl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "de1fe08a-b077-481b-d5f4-afe2afcaf25f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 0.38161117]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgjyQQe5j0vL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#splitting the previous test data into train and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBOSkKkOUhrv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "outputId": "5b1e4d6c-ef48-4f99-adc6-af7258f8abc3"
      },
      "source": [
        "#Applying neural network to newly created dataset. The accuracy of the hybrid\n",
        "#model(model 3) is expected to be better than from the sentiment model(model 1) and \n",
        "#LSTM model (model 2)\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "model3a = Sequential([\n",
        "    Dense(2, activation='relu'),\n",
        "    Dense(8, activation='relu'),\n",
        "    Dense(8, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "model3a.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model3a.fit(X_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=15)\n",
        "          #validation_data=(X_test, y_test))\n",
        "      \n",
        "\n",
        "score, acc = model3a.evaluate(X_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "                            \n",
        "\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "20000/20000 [==============================] - 4s 176us/step - loss: 0.5784 - acc: 0.6882\n",
            "Epoch 2/15\n",
            "20000/20000 [==============================] - 3s 153us/step - loss: 0.4383 - acc: 0.7958\n",
            "Epoch 3/15\n",
            "20000/20000 [==============================] - 3s 154us/step - loss: 0.4262 - acc: 0.7958\n",
            "Epoch 4/15\n",
            "20000/20000 [==============================] - 3s 153us/step - loss: 0.4245 - acc: 0.7958\n",
            "Epoch 5/15\n",
            "20000/20000 [==============================] - 3s 154us/step - loss: 0.4243 - acc: 0.7958\n",
            "Epoch 6/15\n",
            "20000/20000 [==============================] - 3s 153us/step - loss: 0.4242 - acc: 0.7958\n",
            "Epoch 7/15\n",
            "20000/20000 [==============================] - 3s 152us/step - loss: 0.4242 - acc: 0.7958\n",
            "Epoch 8/15\n",
            "20000/20000 [==============================] - 3s 153us/step - loss: 0.4241 - acc: 0.7958\n",
            "Epoch 9/15\n",
            "20000/20000 [==============================] - 3s 154us/step - loss: 0.4241 - acc: 0.7958\n",
            "Epoch 10/15\n",
            "20000/20000 [==============================] - 3s 154us/step - loss: 0.4241 - acc: 0.7958\n",
            "Epoch 11/15\n",
            "20000/20000 [==============================] - 3s 153us/step - loss: 0.4241 - acc: 0.7958\n",
            "Epoch 12/15\n",
            "20000/20000 [==============================] - 3s 152us/step - loss: 0.4241 - acc: 0.7958\n",
            "Epoch 13/15\n",
            "20000/20000 [==============================] - 3s 153us/step - loss: 0.4241 - acc: 0.7958\n",
            "Epoch 14/15\n",
            "20000/20000 [==============================] - 3s 153us/step - loss: 0.4241 - acc: 0.7958\n",
            "Epoch 15/15\n",
            "20000/20000 [==============================] - 3s 153us/step - loss: 0.4241 - acc: 0.7958\n",
            "5000/5000 [==============================] - 0s 81us/step\n",
            "Test score: 0.43907452840805056\n",
            "Test accuracy: 0.7832\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylnjSnjwchmD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "outputId": "30112d91-311e-4fae-ff2a-49ae8f65268b"
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "\n",
        "model3c = Sequential([\n",
        "    Dense(2, activation='relu'),\n",
        "    Dense(8, activation='relu'),\n",
        "    Dense(8, activation='relu'),\n",
        "    Dense(8, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "model3c.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model3c.fit(X_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=15)\n",
        "          #validation_data=(X_test, y_test))\n",
        "      \n",
        "\n",
        "score, acc = model3c.evaluate(X_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "                            \n",
        "\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "20000/20000 [==============================] - 4s 201us/step - loss: 0.6100 - acc: 0.6446\n",
            "Epoch 2/15\n",
            "20000/20000 [==============================] - 3s 168us/step - loss: 0.5235 - acc: 0.7068\n",
            "Epoch 3/15\n",
            "20000/20000 [==============================] - 3s 167us/step - loss: 0.5228 - acc: 0.7068\n",
            "Epoch 4/15\n",
            "20000/20000 [==============================] - 3s 167us/step - loss: 0.5224 - acc: 0.7068\n",
            "Epoch 5/15\n",
            "20000/20000 [==============================] - 3s 167us/step - loss: 0.5226 - acc: 0.7068\n",
            "Epoch 6/15\n",
            "20000/20000 [==============================] - 3s 168us/step - loss: 0.5225 - acc: 0.7068\n",
            "Epoch 7/15\n",
            "20000/20000 [==============================] - 3s 167us/step - loss: 0.4933 - acc: 0.7068\n",
            "Epoch 8/15\n",
            "20000/20000 [==============================] - 3s 167us/step - loss: 0.4108 - acc: 0.7062\n",
            "Epoch 9/15\n",
            "20000/20000 [==============================] - 3s 169us/step - loss: 0.4022 - acc: 0.7160\n",
            "Epoch 10/15\n",
            "20000/20000 [==============================] - 3s 167us/step - loss: 0.0752 - acc: 1.0000\n",
            "Epoch 11/15\n",
            "20000/20000 [==============================] - 3s 168us/step - loss: 0.0026 - acc: 1.0000\n",
            "Epoch 12/15\n",
            "20000/20000 [==============================] - 3s 170us/step - loss: 9.0546e-04 - acc: 1.0000\n",
            "Epoch 13/15\n",
            "20000/20000 [==============================] - 3s 167us/step - loss: 4.5162e-04 - acc: 1.0000\n",
            "Epoch 14/15\n",
            "20000/20000 [==============================] - 3s 167us/step - loss: 2.5766e-04 - acc: 1.0000\n",
            "Epoch 15/15\n",
            "20000/20000 [==============================] - 3s 168us/step - loss: 1.5694e-04 - acc: 1.0000\n",
            "5000/5000 [==============================] - 0s 81us/step\n",
            "Test score: 0.00011903894118731841\n",
            "Test accuracy: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0kptWGY42at",
        "colab_type": "text"
      },
      "source": [
        "## PART 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uslT_Sz0zqb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "5c69aa2e-7f4e-43a9-d567-4e1cd82874fd"
      },
      "source": [
        "imdb_words = imdb.get_word_index()\n",
        "#search for the word in the dictionary to get it's key\n",
        "vec = imdb_words['no']\n",
        "#model2.predict(vec)\n",
        "\n",
        "\n",
        "#Get imdb score, predict on it, calculate the mean of the sentence, if word is not in imdb, returns 0 for that word \n",
        "\n",
        "def get_sentiment_model2(text):\n",
        "  words = text.split()\n",
        "  vects = []\n",
        "  for x in words:\n",
        "    vec = imdb_words.get(x, 0)\n",
        "    vects.append(vec)\n",
        " # print(vects)\n",
        "  pred = model2.predict(vects)\n",
        "  sentiment = np.mean(pred)\n",
        "  return sentiment\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.55395913\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPOuAmRm5LZF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "73180e18-4ef1-4e7a-eb51-3c59170d2a5a"
      },
      "source": [
        "#model 1\n",
        "print(text_to_sentiment('hello, my name is'))\n",
        "\n",
        "#model 2\n",
        "print(get_sentiment_model2('hello, my name is'))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.304105891776296\n",
            "0.55395913\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNavRdQYGnOJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model 3 hybrid predictions \n",
        "#model3.predict(([[1., 0.38161117]]))\n",
        "\n",
        "#function that takes predictions of model1 and model 2 and returns predictions of model 3\n",
        "\n",
        "\n",
        "#Apply the above function the the names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgYujXueILNB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}